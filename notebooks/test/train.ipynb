{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axc_ZAFUykIY"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Masao-Taketani/JPN-EN-Transformer-translator/blob/master/notebooks/test/train.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4v1DluS6axM",
        "outputId": "7b18a1e8-045c-4867-a391-b6dced56f41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6zx8Eaa7-FV",
        "outputId": "3eac0639-7cba-469b-f25e-010c21c792c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmeCRYnj6EDe"
      },
      "source": [
        "import sentencepiece as spm\n",
        "import tensorflow as tf\n",
        "from tensorflow.data.experimental import AUTOTUNE\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZWq4ucyf1J",
        "outputId": "ed5fd9ae-8a59-4d73-b4ee-23f902acb05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "jpn_sp_model = \"drive/My Drive/deep_learning_models/JPN-EN-Transformer-translator/jpn_spm.model\"\n",
        "en_sp_model = \"drive/My Drive/deep_learning_models/JPN-EN-Transformer-translator/en_spm.model\"\n",
        "jpn_sp = spm.SentencePieceProcessor()\n",
        "en_sp = spm.SentencePieceProcessor()\n",
        "jpn_sp.Load(jpn_sp_model)\n",
        "en_sp.Load(en_sp_model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcHSUcptE3xv"
      },
      "source": [
        "def read_data(fpath):\n",
        "  with open(fpath, \"r\") as f:\n",
        "    return f.read()\n",
        "\n",
        "def get_data(fpath):\n",
        "  data = read_data(fpath)\n",
        "  data_list = []\n",
        "  for line in data.split(\"\\n\"):\n",
        "    data_list.append(line)\n",
        "  return data_list\n",
        "\n",
        "def get_max_len_and_list(fpath):\n",
        "  data = read_data(fpath)\n",
        "  max_len = 0\n",
        "  li = []\n",
        "  for line in data.split(\"\\n\"):\n",
        "    li.append(line)\n",
        "    if max_len < len(line):\n",
        "      max_len = len(line)\n",
        "  return max_len, li"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9sc8oKBFGFf",
        "outputId": "0e94dad6-bc5e-45ea-ed98-8769c3bfbf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "jpn_path = \"drive/My Drive/deep_learning_models/JPN-EN-Transformer-translator/jpn_data.txt\"\n",
        "en_path = \"drive/My Drive/deep_learning_models/JPN-EN-Transformer-translator/en_data.txt\"\n",
        "jpn_data = get_data(jpn_path)\n",
        "en_data = get_data(en_path)\n",
        "train_jpn, val_jpn, train_en, val_en = train_test_split(jpn_data, en_data, test_size=0.05)\n",
        "\n",
        "print(\"train size:\", len(train_jpn), \"test size:\", len(test_jpn))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 142298 test size: 7490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlNX0FOuyywQ"
      },
      "source": [
        "### When you have tf.Tensor(string) and .numpy() method is used inside of the tf.py_function, \n",
        "it is converted to just a string. Not a numpy.\n",
        "def encode(jpn, en):\n",
        "  print(\"en\", type(en.numpy()))\n",
        "  jpn_enc = [jpn_sp.PieceToId(\"<s>\")] + jpn_sp.EncodeAsIds(jpn.numpy()) + [jpn_sp.PieceToId(\"</s>\")]\n",
        "  en_enc = [en_sp.PieceToId(\"<s>\")] + en_sp.EncodeAsIds(en.numpy()) + [en_sp.PieceToId(\"</s>\")]\n",
        "  return jpn_enc, en_enc\n",
        "\n",
        "def tf_encode(jpn, en):\n",
        "  print(jpn, en)\n",
        "  result_jpn, result_en = tf.py_function(encode, [jpn, en], [tf.int64, tf.int64])\n",
        "  result_jpn.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "  return result_jpn, result_en"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55hj5_i-k-G",
        "outputId": "32a5c112-b6b5-49d3-ccb2-c7db3f61abb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test\n",
        "t1, t2 = encode(\"こんにちは。今日は比較的涼しい日ですね。\", \"Hello. Today is a relatively cool day, isn't it?\")\n",
        "print(t1)\n",
        "print(t2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 5, 101, 126, 10, 211, 6, 4, 2182, 3176, 7892, 314, 5439, 275, 93, 553, 4, 2]\n",
            "[1, 4241, 4, 1344, 9, 8, 2767, 108, 1625, 103, 13, 9, 42, 10, 19, 29, 20, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvoBNnYUO4G9",
        "outputId": "be97ca43-f7b2-4a46-b4bc-549340ee476d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_jpn, train_en))\n",
        "train_dataset = train_dataset.map(tf_encode)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_jpn, val_en))\n",
        "val_dataset = val_dataset.map(tf_encode)\n",
        "val_dataset = val_dataset.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"args_0:0\", shape=(), dtype=string) Tensor(\"args_1:0\", shape=(), dtype=string)\n",
            "Tensor(\"args_0:0\", shape=(), dtype=string) Tensor(\"args_1:0\", shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hYVybxte6XV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}